# Multi-Agent-Framework-Unittestcase

This repository accompanies the research paper:

AI-Powered Multi-Agent Framework for Automated Unit Test Case Generation: Enhancing Software Quality through LLMs

ðŸ“Œ Paper Overview

This paper presents a closed-loop, AI-powered multi-agent framework for automated unit test case generation and refinement using Large Language Models (LLMs). The framework integrates three specialized agents - Reviewer Agent, Code Generation Agent, and Decision-Making Agent - to iteratively evaluate, improve, and finalize unit test cases with minimal manual intervention.

Unlike traditional prompt-based approaches, this work focuses on the practical evaluation of Generative AI models in real-world software engineering workflows. The proposed method achieves 100% code coverage, robust exception handling, and standards-compliant unit tests, demonstrating measurable improvements in software quality and developer productivity.

ðŸŽ¯ Key Contributions

1. Introduces a multi-agent, LLM-driven workflow for automated unit testing
2. Enables continuous evaluation and refinement of generated test cases
3. Achieves full code coverage with edge-case handling
4. Demonstrates a scalable and repeatable approach suitable for enterprise environments
5. Aligns with trustworthy and evaluative AI principles


ðŸ“Ž Citation

If you use or reference this work, please cite the paper as:

Anusha Garlapati, M. N. V. Satya Sai Muni Parmesh, Dr. Savitha, Jaisri S.
"AI-Powered Multi-Agent Framework for Automated Unit Test Case Generation:
Enhancing Software Quality through LLMs."

